{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.43s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.97s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_3800\\1703765482.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[0mfsod_dataloader\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDataLoader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfsod_dataset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m5\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshuffle\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdrop_last\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 50\u001B[1;33m \u001B[1;32mfor\u001B[0m \u001B[0mimg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfsod_dataloader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     51\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\FRN\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    515\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    516\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 517\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    518\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    519\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[1;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\FRN\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    555\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    556\u001B[0m         \u001B[0mindex\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 557\u001B[1;33m         \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    558\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    559\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\FRN\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     45\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 47\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcollate_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mD:\\Anaconda3\\envs\\FRN\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001B[0m in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m     81\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'each element in list of batch should be of equal size'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     82\u001B[0m         \u001B[0mtransposed\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mbatch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 83\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mdefault_collate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msamples\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0msamples\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtransposed\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     84\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     85\u001B[0m     \u001B[1;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdefault_collate_err_msg_format\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0melem_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\FRN\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     81\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'each element in list of batch should be of equal size'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     82\u001B[0m         \u001B[0mtransposed\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mbatch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 83\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mdefault_collate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msamples\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0msamples\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtransposed\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     84\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     85\u001B[0m     \u001B[1;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdefault_collate_err_msg_format\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0melem_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\FRN\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001B[0m in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m     79\u001B[0m         \u001B[0melem_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     80\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mall\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0melem\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0melem_size\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0melem\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 81\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'each element in list of batch should be of equal size'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     82\u001B[0m         \u001B[0mtransposed\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mbatch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     83\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mdefault_collate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msamples\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0msamples\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtransposed\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "from torchvision.models.detection.rpn import RegionProposalNetwork, AnchorGenerator, RPNHead\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.detection.roi_heads import roi_align\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from models.backbone.ResNet import resnet12\n",
    "from torchvision.models.detection.image_list import ImageList\n",
    "from utils.dataset_tools.coco import read_coco_detection, show_dataset\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# root : coco图片路径\n",
    "# annFile : 标注文件路径\n",
    "# transform : 图像转换(用于PIL)\n",
    "# target_transform : 标注转换\n",
    "# transforms : 图像和标注的转换\n",
    "\n",
    "# 数据集预处理\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "normalize = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=mean, std=std)])\n",
    "t = transforms.Compose([transforms.ToTensor(),\n",
    "                        transforms.Resize(600),  # 短边缩为600\n",
    "                        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "                        transforms.RandomHorizontalFlip(0.5)])\n",
    "\n",
    "# 读取数据集\n",
    "fsod_dataset = read_coco_detection(img_transform=t)\n",
    "\n",
    "# support选取\n",
    "\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "normalize = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=mean, std=std)])\n",
    "transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Resize((600, 600)),\n",
    "                                 transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "                                 transforms.RandomHorizontalFlip(0.5)])\n",
    "fsod_dataset = read_coco_detection(img_transform=transforms)\n",
    "\n",
    "# for i in range(len(fsod_dataset)):\n",
    "#     img, labels = fsod_dataset.__getitem__(i)\n",
    "#     print(img.shape)\n",
    "#     print(len(labels))\n",
    "\n",
    "fsod_dataloader = DataLoader(fsod_dataset, batch_size=5, shuffle=True, drop_last=True)\n",
    "\n",
    "for img, labels in enumerate(fsod_dataloader):\n",
    "    print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# N-way K-shot:\n",
    "# support: N个类别, 每个类别K个实例\n",
    "# query: 不限, 只要在N个类别内即可\n",
    "\n",
    "# 构建N类support, 每个类别的support含有k个实例\n",
    "\n",
    "def generate_support():\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def crop_support(self, roidb):\n",
    "    # Get support box\n",
    "    '''\n",
    "    data_height, data_width = map(int, blobs['im_info'][:2])\n",
    "    im_scale = blobs['im_info'][-1]\n",
    "    #print(blobs['roidb'])\n",
    "    print(blobs['im_info'])\n",
    "    print('support data:', blobs['data'].shape)\n",
    "    all_box = (blobs['roidb'][0]['boxes'] * im_scale).astype(np.int16)\n",
    "    '''\n",
    "    img_path = roidb[0]['image']\n",
    "    all_box = roidb[0]['boxes']\n",
    "    all_cls = np.array(roidb[0]['gt_classes'])\n",
    "    target_cls = roidb[0]['target_cls']\n",
    "\n",
    "    target_idx = np.where(all_cls == target_cls)[0]\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    if roidb[0]['flipped']:\n",
    "        img = img[:, ::-1, :]\n",
    "    img = img.astype(np.float32, copy=False)\n",
    "    # img -= cfg.PIXEL_MEANS # np.array([[[102.9801, 115.9465, 122.7717]]])\n",
    "    img -= np.array([[[102.9801, 115.9465, 122.7717]]])  # np.array([[[102.9801, 115.9465, 122.7717]]])\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    data_height = int(img.shape[1])\n",
    "    data_width = int(img.shape[2])\n",
    "\n",
    "    all_box_num = all_box.shape[0]\n",
    "    picked_box_id = np.random.choice(target_idx)  #random.choice(range(all_box_num))\n",
    "    #print(all_cls, target_cls, target_idx, picked_box_id)\n",
    "    picked_box = all_box[picked_box_id, :][np.newaxis, :].astype(np.int16)\n",
    "    #print('1', blobs['roidb'][0]['boxes'], '2', roidb[0]['boxes'], '3', picked_box)\n",
    "    '''\n",
    "    original_img = cv2.imread(img_path)\n",
    "    if roidb[0]['flipped']:\n",
    "        original_img = original_img[:, ::-1, :]\n",
    "    original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "    self.vis_image(original_img, picked_box[0], img_path.split('/')[-1][:-4] + '_real.jpg', './test')\n",
    "    '''\n",
    "    '''\n",
    "    picked_box = (picked_box / im_scale).astype(np.int16)\n",
    "    data_height = int(data_height / im_scale)\n",
    "    data_width = int(data_width / im_scale)\n",
    "    blobs['data'] = cv2.resize(blobs['data'], (data_height, data_width))\n",
    "    '''\n",
    "    #print('3', picked_box)\n",
    "    x1 = picked_box[0][0]\n",
    "    y1 = picked_box[0][1]\n",
    "    x2 = picked_box[0][2]\n",
    "    y2 = picked_box[0][3]\n",
    "\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    context_pixel = 16  #int(16 * im_scale)\n",
    "\n",
    "    new_x1 = 0\n",
    "    new_y1 = 0\n",
    "    new_x2 = width\n",
    "    new_y2 = height\n",
    "\n",
    "    target_size = (320, 320)  #(384, 384)\n",
    "\n",
    "    if width >= height:\n",
    "        crop_x1 = x1 - context_pixel\n",
    "        crop_x2 = x2 + context_pixel\n",
    "\n",
    "        # New_x1 and new_x2 will change when crop context or overflow\n",
    "        new_x1 = new_x1 + context_pixel\n",
    "        new_x2 = new_x1 + width\n",
    "        if crop_x1 < 0:\n",
    "            new_x1 = new_x1 + crop_x1\n",
    "            new_x2 = new_x1 + width\n",
    "            crop_x1 = 0\n",
    "        if crop_x2 > data_width:\n",
    "            crop_x2 = data_width\n",
    "\n",
    "        short_size = height\n",
    "        long_size = crop_x2 - crop_x1\n",
    "        y_center = int((y2 + y1) / 2)  #math.ceil((y2 + y1) / 2)\n",
    "        crop_y1 = int(y_center - (long_size / 2))  #int(y_center - math.ceil(long_size / 2))\n",
    "        crop_y2 = int(y_center + (long_size / 2))  #int(y_center + math.floor(long_size / 2))\n",
    "\n",
    "        # New_y1 and new_y2 will change when crop context or overflow\n",
    "        new_y1 = new_y1 + math.ceil((long_size - short_size) / 2)\n",
    "        new_y2 = new_y1 + height\n",
    "        if crop_y1 < 0:\n",
    "            new_y1 = new_y1 + crop_y1\n",
    "            new_y2 = new_y1 + height\n",
    "            crop_y1 = 0\n",
    "        if crop_y2 > data_height:\n",
    "            crop_y2 = data_height\n",
    "\n",
    "        crop_short_size = crop_y2 - crop_y1\n",
    "        crop_long_size = crop_x2 - crop_x1\n",
    "        square = np.zeros((3, crop_long_size, crop_long_size), dtype=np.float32)\n",
    "        delta = int((crop_long_size - crop_short_size) / 2)  #int(math.ceil((crop_long_size - crop_short_size) / 2))\n",
    "        square_y1 = delta\n",
    "        square_y2 = delta + crop_short_size\n",
    "\n",
    "        new_y1 = new_y1 + delta\n",
    "        new_y2 = new_y2 + delta\n",
    "\n",
    "        crop_box = img[:, crop_y1:crop_y2, crop_x1:crop_x2]\n",
    "\n",
    "        square[:, square_y1:square_y2, :] = crop_box\n",
    "\n",
    "        #show_square = np.zeros((crop_long_size, crop_long_size, 3))#, dtype=np.int16)\n",
    "        #show_crop_box = original_img[crop_y1:crop_y2, crop_x1:crop_x2, :]\n",
    "        #show_square[square_y1:square_y2, :, :] = show_crop_box\n",
    "        #show_square = show_square.astype(np.int16)\n",
    "    else:\n",
    "        crop_y1 = y1 - context_pixel\n",
    "        crop_y2 = y2 + context_pixel\n",
    "\n",
    "        # New_y1 and new_y2 will change when crop context or overflow\n",
    "        new_y1 = new_y1 + context_pixel\n",
    "        new_y2 = new_y1 + height\n",
    "        if crop_y1 < 0:\n",
    "            new_y1 = new_y1 + crop_y1\n",
    "            new_y2 = new_y1 + height\n",
    "            crop_y1 = 0\n",
    "        if crop_y2 > data_height:\n",
    "            crop_y2 = data_height\n",
    "\n",
    "        short_size = width\n",
    "        long_size = crop_y2 - crop_y1\n",
    "        x_center = int((x2 + x1) / 2)  #math.ceil((x2 + x1) / 2)\n",
    "        crop_x1 = int(x_center - (long_size / 2))  #int(x_center - math.ceil(long_size / 2))\n",
    "        crop_x2 = int(x_center + (long_size / 2))  #int(x_center + math.floor(long_size / 2))\n",
    "\n",
    "        # New_x1 and new_x2 will change when crop context or overflow\n",
    "        new_x1 = new_x1 + math.ceil((long_size - short_size) / 2)\n",
    "        new_x2 = new_x1 + width\n",
    "        if crop_x1 < 0:\n",
    "            new_x1 = new_x1 + crop_x1\n",
    "            new_x2 = new_x1 + width\n",
    "            crop_x1 = 0\n",
    "        if crop_x2 > data_width:\n",
    "            crop_x2 = data_width\n",
    "\n",
    "        crop_short_size = crop_x2 - crop_x1\n",
    "        crop_long_size = crop_y2 - crop_y1\n",
    "        square = np.zeros((3, crop_long_size, crop_long_size), dtype=np.float32)\n",
    "        delta = int((crop_long_size - crop_short_size) / 2)  #int(math.ceil((crop_long_size - crop_short_size) / 2))\n",
    "        square_x1 = delta\n",
    "        square_x2 = delta + crop_short_size\n",
    "\n",
    "        new_x1 = new_x1 + delta\n",
    "        new_x2 = new_x2 + delta\n",
    "\n",
    "        crop_box = img[:, crop_y1:crop_y2, crop_x1:crop_x2]\n",
    "        square[:, :, square_x1:square_x2] = crop_box\n",
    "\n",
    "        #show_square = np.zeros((crop_long_size, crop_long_size, 3)) #, dtype=np.int16)\n",
    "        #show_crop_box = original_img[crop_y1:crop_y2, crop_x1:crop_x2, :]\n",
    "        #show_square[:, square_x1:square_x2, :] = show_crop_box\n",
    "        #show_square = show_square.astype(np.int16)\n",
    "\n",
    "    square = square.astype(np.float32, copy=False)\n",
    "    square_scale = float(target_size[0]) / long_size\n",
    "    square = square.transpose(1, 2, 0)\n",
    "    square = cv2.resize(square, target_size,\n",
    "                        interpolation=cv2.INTER_LINEAR)  # None, None, fx=square_scale, fy=square_scale, interpolation=cv2.INTER_LINEAR)\n",
    "    square = square.transpose(2, 0, 1)\n",
    "\n",
    "    new_x1 = int(new_x1 * square_scale)\n",
    "    new_y1 = int(new_y1 * square_scale)\n",
    "    new_x2 = int(new_x2 * square_scale)\n",
    "    new_y2 = int(new_y2 * square_scale)\n",
    "\n",
    "    # For test\n",
    "    #show_square = cv2.resize(show_square, target_size, interpolation=cv2.INTER_LINEAR) # None, None, fx=square_scale, fy=square_scale, interpolation=cv2.INTER_LINEAR)\n",
    "    #self.vis_image(show_square, [new_x1, new_y1, new_x2, new_y2], img_path.split('/')[-1][:-4]+'_crop.jpg', './test')\n",
    "\n",
    "    support_data = square\n",
    "    support_box = np.array([[new_x1, new_y1, new_x2, new_y2]]).astype(np.float32)\n",
    "    return support_data, support_box"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.86s)\n",
      "creating index...\n",
      "index created!\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=333x500 at 0x218889A5748> datasets/fsod/images/part_1/n00480993/n00480993_11037.jpg\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from utils.dataset_tools.coco import read_coco_detection\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def class_dict(cocoDataset: Dataset):\n",
    "    class_dicts = {}\n",
    "    for img, labels in enumerate(cocoDataset):\n",
    "        print(img, labels)\n",
    "        for label in labels:\n",
    "            class_index = label['category_id']\n",
    "            if not class_index in class_dicts.keys():\n",
    "                class_dicts[class_index] = []\n",
    "                class_dicts[class_index].append(img.filename)\n",
    "            else:\n",
    "                if not img.filename in class_dicts[class_index]:\n",
    "                    class_dicts[class_index].append(img.filename)\n",
    "    json.dump(class_dicts, fp='class_dicts')\n",
    "\n",
    "\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "# normalize = transforms.Compose([transforms.ToTensor(),\n",
    "#                                 transforms.Normalize(mean=mean, std=std)])\n",
    "# transforms = transforms.Compose([transforms.ToTensor(),\n",
    "#                                  transforms.Resize((600, 600)),\n",
    "#                                  transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "#                                  transforms.RandomHorizontalFlip(0.5)])\n",
    "fsod_dataset = read_coco_detection(img_transform=None)\n",
    "\n",
    "# for i in range(len(fsod_dataset)):\n",
    "#     img, labels = fsod_dataset.__getitem__(i)\n",
    "#     print(img.shape)\n",
    "#     print(len(labels))\n",
    "\n",
    "class_dict(fsod_dataset)\n",
    "\n",
    "img = Image.open('datasets/fsod/images/part_1/n00480993/n00480993_11037.jpg')\n",
    "print(img, img.filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}